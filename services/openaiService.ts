import OpenAI from "openai";
import { LocationItem } from "../types";
import { getSettings } from "../utils/storage";

// Helper to get client with dynamic settings
const getOpenAIClient = () => {
  const settings = getSettings();
  
  if (!settings.llmApiKey) {
    throw new Error("è¯·å…ˆåœ¨è®¾ç½®ä¸­é…ç½® LLM API Key");
  }

  const options: any = {
    apiKey: settings.llmApiKey,
    dangerouslyAllowBrowser: true, // å…è®¸åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨
  };

  // æ”¯æŒè‡ªå®šä¹‰base URL (ç”¨äºæœ¬åœ°æ¨¡å‹)
  if (settings.llmBaseUrl) {
    options.baseURL = settings.llmBaseUrl;
  }
  
  return new OpenAI(options);
};

// Extract locations from text using OpenAI-compatible API
export const extractLocationsFromText = async (text: string): Promise<Omit<LocationItem, 'lat' | 'lng'>[]> => {
  console.log('ğŸ” [DEBUG] å¼€å§‹LLMåœ°ç‚¹æå–...');
  console.log('ğŸ“ [DEBUG] å¾…è§£ææ–‡æœ¬:', text);
  
  const client = getOpenAIClient();
  const settings = getSettings();
  
  const prompt = `
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…·ä½“çš„åœ°ç‚¹ä¿¡æ¯ã€‚
    å¯¹äºæ¯ä¸ªåœ°ç‚¹ï¼Œç¡®å®šå…¶åç§°ã€æ‰€åœ¨çš„åŸå¸‚ã€ç±»å‹ï¼ˆæ™¯ç‚¹ spot / ç¾é£Ÿ food / ä½å®¿ hotel / å…¶ä»– otherï¼‰ä»¥åŠç›¸å…³çš„ä¸Šä¸‹æ–‡æè¿°ã€‚
    ä¸è¦ç¼–é€ åæ ‡ï¼Œåæ ‡å°†ç”±åœ°å›¾æœåŠ¡åç»­æä¾›ã€‚
    
    è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªåœ°ç‚¹å¯¹è±¡åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    - name: åœ°ç‚¹åç§°ï¼Œä¾‹å¦‚ 'éƒ½æ±Ÿå °æ™¯åŒº'
    - city: åŸå¸‚åç§°ï¼Œä¾‹å¦‚ 'æˆéƒ½'
    - type: ç±»å‹ï¼Œæšä¸¾å€¼ ['spot', 'food', 'hotel', 'other']
    - context: åŸæ–‡ä¸­å…³äºè¯¥åœ°ç‚¹çš„æè¿°
    
    å¾…è§£ææ–‡æœ¬: "${text}"
    
    è¯·ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
  `;

  console.log('ğŸ¤– [DEBUG] LLMè¯·æ±‚é…ç½®:', {
    model: settings.llmModel || 'gpt-3.5-turbo',
    baseURL: settings.llmBaseUrl || 'é»˜è®¤',
    temperature: 0.1,
    response_format: 'json_object'
  });
  console.log('ğŸ’¬ [DEBUG] å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯:', prompt);

try {
    console.log('â³ [DEBUG] æ­£åœ¨è°ƒç”¨LLM API...');
    const startTime = Date.now();
    
    // å°è¯•æ–¹æ³•1: ä½¿ç”¨æ–°çš„response_formatæ ¼å¼
    let response;
    try {
      console.log('ğŸ”„ [DEBUG] å°è¯•æ–¹æ³•1: æ–°ç‰ˆresponse_formatæ ¼å¼');
      response = await client.chat.completions.create({
        model: settings.llmModel || 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åœ°ç‚¹æå–åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¿”å›ç»“æœã€‚'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.1,
        response_format: { type: "json_object" }
      });
    } catch (error1: any) {
      console.warn('âš ï¸ [DEBUG] æ–°ç‰ˆæ ¼å¼å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:', error1.message);
      
      // å¦‚æœæ˜¯response_formatç›¸å…³çš„é”™è¯¯ï¼Œå°è¯•å›é€€åˆ°é€šç”¨æ ¼å¼
      if (error1.message?.includes('response_format') || error1.message?.includes('BadRequestError')) {
        console.log('ğŸ”„ [DEBUG] å°è¯•æ–¹æ³•2: é€šç”¨æ ¼å¼ï¼ˆæ— response_formatçº¦æŸï¼‰');
        response = await client.chat.completions.create({
          model: settings.llmModel || 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åœ°ç‚¹æå–åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: 0.1
        });
      } else {
        // å¦‚æœä¸æ˜¯response_formaté”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
        throw error1;
      }
    }

    const endTime = Date.now();
    console.log(`âœ… [DEBUG] LLM APIè°ƒç”¨å®Œæˆï¼Œè€—æ—¶ ${endTime - startTime}ms`);
    
    const content = response.choices[0]?.message?.content || "[]";
    console.log('ğŸ“„ [DEBUG] LLMåŸå§‹å›å¤å†…å®¹:', content);
    console.log('ğŸ“Š [DEBUG] LLMå›å¤ç»Ÿè®¡:', {
      choicesCount: response.choices?.length,
      firstChoice: response.choices[0]?.message?.role,
      contentLength: content?.length,
      finishReason: response.choices[0]?.finish_reason
    });
    
    let data: any;
    try {
      data = JSON.parse(content);
      console.log('ğŸ”„ [DEBUG] JSONè§£ææˆåŠŸ:', data);
    } catch (parseError) {
      console.error('âŒ [DEBUG] JSONè§£æå¤±è´¥:', parseError);
      console.log('ğŸ“„ [DEBUG] å°è¯•è§£æçš„å†…å®¹:', content);
      throw new Error('LLMè¿”å›çš„JSONæ ¼å¼æ— æ•ˆ');
    }
    
    // Handle both array and object responses
    const locationsArray = Array.isArray(data) ? data : (data.locations || data.data || []);
    console.log('ğŸ“ [DEBUG] æå–åˆ°çš„åœ°ç‚¹æ•°é‡:', locationsArray.length);
    console.log('ğŸ“‹ [DEBUG] æå–åˆ°çš„åœ°ç‚¹è¯¦æƒ…:', locationsArray);
    
    // Add client-side IDs
    const result = locationsArray.map((item: any) => ({
      ...item,
      id: crypto.randomUUID()
    }));
    
    console.log('ğŸ·ï¸ [DEBUG] æ·»åŠ IDåçš„åœ°ç‚¹åˆ—è¡¨:', result);
    console.log('âœ… [DEBUG] LLMåœ°ç‚¹æå–å®Œæˆ');
    
    return result;
  } catch (error: any) {
    console.error('âŒ [DEBUG] OpenAI/LLMæå–å¤±è´¥:', error);
    console.error('âŒ [DEBUG] é”™è¯¯è¯¦æƒ…:', {
      message: error.message,
      stack: error.stack,
      name: error.name
    });
    throw new Error(`åœ°ç‚¹æå–å¤±è´¥: ${error.message || "æœªçŸ¥é”™è¯¯"}`);
  }
};

export const generateRouteAdvice = async (locations: LocationItem[], totalMinutes: number): Promise<string> => {
  console.log('ğŸ¤– [DEBUG] å¼€å§‹ç”ŸæˆAIè¡Œç¨‹å»ºè®®...');
  console.log('ğŸ“ [DEBUG] è¡Œç¨‹åœ°ç‚¹æ•°é‡:', locations.length);
  console.log('â±ï¸ [DEBUG] æ€»æ—¶é•¿:', `${Math.round(totalMinutes)}åˆ†é’Ÿ`);
  
  const client = getOpenAIClient();
  const settings = getSettings();

  const locString = locations.map((l, index) =>
    `${index + 1}. ${l.name} (${l.type}) - ${l.context}`
  ).join('\n');
  
  console.log('ğŸ“‹ [DEBUG] è¡Œç¨‹åœ°ç‚¹åˆ—è¡¨:', locString);

  const prompt = `
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¼æ¸¸ã€‚ä»¥ä¸‹æ˜¯å·²ç»æŒ‰æœ€çŸ­è·¯å¾„è§„åˆ’å¥½çš„è¡Œç¨‹é¡ºåºï¼Œæ€»é¢„ä¼°é€šå‹¤æ—¶é—´ä¸º ${Math.round(totalMinutes)} åˆ†é’Ÿï¼š
    ${locString}
    
    è¯·ç”¨ä¸­æ–‡ç”Ÿæˆä¸€æ®µç®€çŸ­ã€è¿è´¯çš„è¡Œç¨‹å»ºè®®ã€‚
    é£æ ¼è¦æ±‚ï¼šæç®€ã€å¹²ç»ƒã€å®ä½“æ„Ÿã€‚ä¾‹å¦‚ï¼š"å»ºè®®æ—©ä¸Šå…ˆå»Aï¼Œä¸­åˆåœ¨Båƒé¥­..."ã€‚
    å­—æ•°æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚
  `;

  console.log('ğŸ’¬ [DEBUG] AIå»ºè®®æç¤ºè¯:', prompt);
  console.log('ğŸ”§ [DEBUG] AIæ¨¡å‹é…ç½®:', {
    model: settings.llmModel || 'gpt-3.5-turbo',
    temperature: 0.7,
    max_tokens: 200
  });

try {
    console.log('â³ [DEBUG] æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆå»ºè®®...');
    const startTime = Date.now();
    
    // ç›´æ¥ä½¿ç”¨é€šç”¨æ ¼å¼ï¼Œé¿å…response_formatå…¼å®¹æ€§é—®é¢˜
    const response = await client.chat.completions.create({
      model: settings.llmModel || 'gpt-3.5-turbo',
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œå¯¼æ¸¸ï¼Œæ“…é•¿æä¾›ç®€æ´å®ç”¨çš„è¡Œç¨‹å»ºè®®ã€‚'
        },
        {
          role: 'user',
          content: prompt
        }
      ],
      temperature: 0.7,
      max_tokens: 200
    });

    const endTime = Date.now();
    console.log(`âœ… [DEBUG] AIå»ºè®®ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ ${endTime - startTime}ms`);
    
    const advice = response.choices[0]?.message?.content || "æš‚æ— è¡Œç¨‹å»ºè®®";
    console.log('ğŸ’¡ [DEBUG] AIç”Ÿæˆçš„å»ºè®®:', advice);
    
    return advice;
  } catch (error: any) {
    console.error('âŒ [DEBUG] AIå»ºè®®ç”Ÿæˆå¤±è´¥:', error);
    console.error('âŒ [DEBUG] é”™è¯¯è¯¦æƒ…:', {
      message: error.message,
      stack: error.stack
    });
    return "è¡Œç¨‹å·²ç”Ÿæˆï¼Œä½†å»ºè®®åŠ è½½å¤±è´¥ã€‚";
  }
};